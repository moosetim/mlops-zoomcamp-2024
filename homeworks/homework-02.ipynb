{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Zoomcamp 2024\n",
    "\n",
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used GitHub Codespaces to open the [repository](https://github.com/moosetim/mlops-zoomcamp-2024) in my local VS Code.\n",
    "\n",
    "I used the following commands to set up a Conda virtual environment and install MLflow along with other required packages listed in the `requirements.txt` file: \n",
    "1. `conda create -n exp-tracking-env python=3.9`\n",
    "2. `conda init`\n",
    "3. `conda activate exp-tracking-env`\n",
    "4. `pip install -r requirements.txt`\n",
    "\n",
    "The output of `mlflow --version`: `mlflow, version 2.13.0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Green Taxi Trip Records datasets for January, February, March 2023 (`green_tripdata_2023-*.parquet`) were saved in the folder `02-experiment-tracking/data`. The preprocessing script preprocess_data.py was saved in the `homeworks` folder.\n",
    "\n",
    "The following command was used to execute the script: `python preprocess_data.py --raw_data_path ../02-experiment-tracking/data/ --dest_path ./output`. \n",
    "\n",
    "The created `output` folder has **4 files** (`dv.pkl`, `test.pkl`, `train.pkl`, `val.pkl`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in the terminal to launch the MLflow UI in the browser and specify the backend store uri: `mlflow ui --backend-store-uri sqlite:///mlflow.db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/workspaces/mlops-zoomcamp-2024/homeworks/mlruns/1', creation_time=1716504547872, experiment_id='1', last_update_time=1716504547872, lifecycle_stage='active', name='hw-2-experiment', tags={}>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"hw-2-experiment\") # MLflow will assign all the runs to this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code were added to the `train.py` file to enable MLflow autologging:\n",
    "\n",
    "    import mlflow\n",
    "\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"hw-2-experiment\") \n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Enable auto-logging\n",
    "        mlflow.autolog()\n",
    "\n",
    "        # Run the training script\n",
    "        run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the model runs in the MLflow UI, **the value of the min_samples_split parameter is 2**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following command to launch the MLflow server: `mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script `hpo.py` has been updated and can be found in the `homeworks` folder. The main changes were made to the `objectiv()` function:\n",
    "\n",
    "        def objective(params):\n",
    "            with mlflow.start_run(nested=True):\n",
    "\n",
    "                # Log the parameters to MLflow\n",
    "                mlflow.log_params(params)\n",
    "                \n",
    "                rf = RandomForestRegressor(**params)\n",
    "                rf.fit(X_train, y_train)\n",
    "                y_pred = rf.predict(X_val)\n",
    "                \n",
    "                rmse = root_mean_squared_error(y_val, y_pred)\n",
    "                # Log RMSE to MLflow\n",
    "                mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "            return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "To check the best RMSE, launch the mlflow UI with and rank the runs in order of the increasing RMSE. The lowest RMSE equals to `5.335419588556921`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following changes were made in the `register_model.py` file which is located in the `homeworks` folder:\n",
    "1. Additional `print()` statements were added to faciliate debugging\n",
    "2. For the model from the best run, the lowest test RMSE from the best model was printed out and further confirmed in the MLflow UI.\n",
    "\n",
    "The lowest test RMSE was `5.567408012462019`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-tracking-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
